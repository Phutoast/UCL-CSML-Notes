\section{Learning Theory}

\subsection{Introduction}

\begin{definition}{\textbf{(Distribution over Subset)}}
    If $\mathcal{D}$ is a distribution over $\mathcal{Z}$ then if $A\subseteq \mathcal{Z}$ then $\mathcal{D}(A)$ denotes the probability that if $z$ is drawn from $\mathcal{D}$ that $z \in A$
\end{definition}

\begin{definition}{\textbf{(Expected Error)}}
    Data is sampled iid from a distribution $\mathcal{D}$ over $\mathcal{X}\times\mathcal{Y}$ with $\mathcal{Y} = \brackc{0, 1}$. The expected error of function $h : \mathcal{X}\rightarrow\mathcal{Y}$ is:
    \begin{equation*}
        L_\mathcal{D}(h) = \mathcal{D}(\brackc{x, y} : h(x)\ne y) = \mathbb{P}_{(x, y)\sim\mathcal{D}}[h(x)\ne y] = \int \mathbb{I}[h(x) \ne y]\dby p(x, y)
    \end{equation*}
    where we denote $L_\mathcal{D}(h) = \mathcal{E}(h)$
\end{definition}

\begin{definition}{\textbf{(Empirical Error)}}
   The empirical error of $h$ given the dataset $S = \brackc{(\boldsymbol x_1, y_1),\dots,(\boldsymbol x_m, y_m)}$ is denoted as:
   \begin{equation*}
       L_S(h) = \frac{1}{m}\sum^m_{i=1}\mathbb{I}[h(x_i) \ne y_i]
   \end{equation*}
   Or, we denote it as $\mathcal{E}_\text{emp}(S, h)$.
\end{definition}

\begin{theorem}{\textbf{(Hoeffding's Inequality)}}
    Let $Z_1,Z_2,\dots,Z_m$ be iid bernoulli random variable, when for all $i$, we have $\mathbb{P}(Z_i = 1) = p$ and let $\bar{Z} = 1/m\sum^m_{i=1}Z_i$, then for any $\varepsilon > 0$ as we have:
    \begin{equation*}
        \mathbb{P}(\bar{Z} > p +\varepsilon) \le \exp(-2m\varepsilon^2) \qquad \mathbb{P}(\bar{Z} < p - \varepsilon) \le \exp(-2m\varepsilon^2)
    \end{equation*}
\end{theorem}

\begin{theorem}
    Select a function $h$ then for any $\delta\in(0, 1)$ with probability $1-\delta$ over the random sample $V$ of size $m$ from $\mathcal{D}$, we have:
    \begin{equation*}
        L_\mathcal{D}(h) \le L_V(h)+\sqrt{\frac{\ln(1/\delta)}{2m}}
    \end{equation*}
    The generalization error of a function $h$ may be bounded by the empirical error. We may select a predictor $h$ on any set $S$, as we may bound it on the validation on separate set of data $V$.
\end{theorem}
\begin{proof}
    Given a predictor $h$, we have the differences to be:
    \begin{equation*}
        L_\mathcal{D}(h) - L_V(h) = \mathbb{P}_{(x, y)\sim\mathcal{D}}[h(x)\ne y] - \frac{1}{m}\sum^m_{i=1}\mathbb{I}[h(x_i)\ne y_i]
    \end{equation*}
    we can define $Z_i = \mathbb{I}[h(x_i)\ne y_i]$. We can see that $Z_1,\dots,Z_m$ are statistical independent. Then for all $\mathbb{P}[Z_i] = L_\mathcal{D}(h) = \mathbb{P}[h(x)\ne y]$. We apply the Hoeffding inequality, gives us:
    \begin{equation*}
        \mathbb{P}[L_\mathcal{D}(h) - L_V(h)\ge\varepsilon]\le\exp(2-\varepsilon^2)
    \end{equation*}
    setting $\delta = \exp(-2\varepsilon^2m)$, and solving this gives us the theorem. 
\end{proof}

\begin{remark}
    If we use the upper and lower bound $m$, the Hoeffding inequality would gives us:
    \begin{equation*}
        \abs{L_\mathcal{D}(h) - L_V(h)} \le \sqrt{\frac{\ln(2/\delta)}{2m}}
    \end{equation*}
    This is a nice result, but there are some drawnbacks to this bound:
    \begin{itemize}
        \item The validation bound gives a way to estimate of the confidence interval for the generalization error. The data $V$ can't be used for training. 
        \item Having small number of data, can we choose a model based on the expected error directly, without the training data ?
        \item The bound is about the predictor, while we need to analyze the prediction done by the machine learning algorithm. 
    \end{itemize}
\end{remark}

\begin{definition}{\textbf{(General Statistical Consider)}}
    Statistical model begin with an assumption that the data is generated by the underlying distribution $\mathcal{D}$ not known to the learner. Assuming that we are given a training set that is generated iid from distribution $\mathcal{D}$:
    \begin{equation*}
        S = \brackc{(x_1, y_1),\dots,(x_m, y_m)}
    \end{equation*}
\end{definition}

\begin{definition}{\textbf{(Empirical Risk Mimization)}}
    Assuming we have a learning algorithm $A$ that chooses a hypothesis function $A_\mathcal{H}(S)$ from a hypothesis space $\mathcal{H}$ in response to the training set $S$. We study the ERM:
    \begin{equation*}
        \operatorname{ERM}_\mathcal{H}(S) = \argmin{h\in\mathcal{H}}L_S(h)
    \end{equation*}
    There are many possible empirical minimizer as we assume ERM to be an arbitrary one. 
\end{definition}

\begin{remark}
    The traditional statistic $h_S$ concentrated on analysing:
    \begin{equation*}
        \lim_{m\rightarrow\infty}\mathbb{E}_{S_m}[L_\mathcal{D}(A(S_m))]
    \end{equation*}
    where $S_m$ denotes a training set of size $m$. For finite sample, the generalization $L_D(A(S_m))$ has a distribution depending on the algorithm and function class and sample size:
    \begin{itemize}
        \item \emph{Traditional Statistic}: concentrated on the mean of this distribution but this quantity is misleading for example in the case of low fold cross-validation. 
        \item \emph{Statistical Learning Theory}: analyze the tail of the distribution finding and the bound that holds in high probability. 
    \end{itemize}
\end{remark}

\begin{definition}{\textbf{(Reliability Assumption)}}
    Assume that there exists a function $f^*$ so that for all $x \in \mathcal{X}$, we have $f^*(x)=y$ there exists a classifier that has zero error. We can now take $\mathcal{D}$ to be only a distribution over $\mathcal{X}$ only. We consider the following loss:
    \begin{equation*}
        L_{\mathcal{D}, f^*}(h) = \mathbb{P}[h(x) \ne f^*(x)]
    \end{equation*}
    We can find the algorithm $A$ so that $h=A(S)$ such that $L_{\mathcal{D}, f^*}(h)=0$ is small. 
\end{definition}

\begin{remark}{\textbf{(Reason for Approximation)}}
    We can't hope the find the function $h$ such that $L_{\mathcal{D}, f^*}(h) = 0$ Let's consider the $\varepsilon \in (0, 1)$ that takes $\mathcal{X} = \brackc{x_1,x_2}$ where $\mathcal{D}(\brackc{x_1}) = 1-\varepsilon$ and $\mathcal{D}(\brackc{x_2}) = \varepsilon$:
    \begin{itemize}
        \item The probability to not see $x_2$ at all among $m$ iid example is $(1-\varepsilon)^m \approx \exp(-\varepsilon m)$
        \item If $\varepsilon\ll1/m$, we are unlikely to see $x_2$ at all. then we don't know its label.
    \end{itemize}
    So, we are only happy to see $L_{\mathcal{D}, f^*}(h)\le\varepsilon$ when $\varepsilon$ is user defined.
\end{remark}

\begin{remark}{\textbf{(Reason for Probability)}}
    The input is randomly generated (there is a small chance that we will see the same sample over and over again). No algorithm can generate $L_{\mathcal{D}, f^*}(h)\le\varepsilon$ for sure, and so we allow the algorithm to fail with some probability $\delta\in(0, 1)$ that is user defined. 
\end{remark}

\begin{definition}{\textbf{(PAC Learning)}}
    The learner doesn't know $\mathcal{D}$ and $f^*$. It receives parameter $\varepsilon$ and $\delta$. Learner can aske for training data $S$ contrary for $m(\varepsilon, \delta)$ examples. The learner should output a hypothesis $h$ such that with at least probability $1-\delta$, it holds that $L_{\mathcal{D}, f^*}(h)\le\varepsilon$
\end{definition}

\begin{theorem}{\textbf{(No Free Lunch)}} 
    \begin{itemize}
        \item Suppose $\abs{\mathcal{X}}=\infty$. For any fixed $C\subset\mathcal{X}$ take $\mathcal{D}$ to be uniform $m$ distribution over $C$:
        \item If the number of training example is $m \le |C|/2$, the learner has no knowledge of at least half of elements in $C$. 
    \end{itemize}
    Fix $\delta\in(0, 1)$ and $\varepsilon<1/2$. For any learner $A$ and training set of size $m$, there exists $\mathcal{D}$ and $f^*$ such that with probability $\delta$ over the generation of a training data $S$ of $m$ examples, it holds that
    \begin{equation*}
        L_{\mathcal{D}, f^*}(A(S))\ge\varepsilon
    \end{equation*}
\end{theorem}
\begin{proof}
    Consider for contradiction, assuming that the class is learnable, consider $\varepsilon > 1/8$ and $\delta \le 1/7$. With the definition of PAC learnable $m(\varepsilon, \delta) = m$:
    \begin{itemize}
        \item For the consistent case, with probability greater than $1-\delta$, when $A$ is applied to sample $S$ of size $m$, generated iid $\mathcal{D}$, we have
        \begin{equation*}
            L_\mathcal{D, f^*}(A(S))  \le \varepsilon
        \end{equation*}
        \item However, using the NFL thoerem above, since $|\mathcal{X}|>2m$, for every learning algorithm, there exists a $\mathcal{D}$ such that with probability greater than $1/7>\delta$, and $L_\mathcal{D, f^*}(A(S)) > 1/8>\varepsilon$
    \end{itemize}
    This is a contradiction. 
\end{proof}

\subsection{PAC of Finite Hypothesis Class}

\begin{lemma}
    For any $2$ sets $A$ and $B$, and a distribution $\mathcal{D}$ we can show that:
    \begin{equation*}
        \mathcal{D}(A\cup B) \le \mathcal{D}(A) + \mathcal{D}(B)
    \end{equation*}
\end{lemma}

\begin{theorem}
    Fix $\varepsilon, \delta$. If we have $m\ge\log(\abs{\mathcal{H}}/\delta)/\varepsilon$, then for every $\mathcal{D}, f^*$ with probability of at least $1-\delta$ (with respected to randomly sample training set $S$ of size $m$), we now have:
    \begin{equation*}
        L_{\mathcal{D}, f^*} (\text{ERM}_\mathcal{H}(S)) \le \varepsilon
    \end{equation*}
    This mean that we have $L_{\mathcal{D}, f^*} (\text{ERM}_\mathcal{H}(S)) \le (\log\abs{\mathcal{H}} + \log(1/\delta))/m$. The generalization error decrease linear in the number of samples and increase in logarithm in the size of hypothesis class.
\end{theorem}
\begin{proof}
    Consider $S|_{x} = (x_1,\dots,x_m)$ be instances of training set. We will show that:
    \begin{equation*}
        \mathcal{D}^m(\brackc{S|_x : L_{\mathcal{D}, f^*} (\text{ERM}_\mathcal{H}(S)) > \varepsilon}) \le \delta
    \end{equation*}
    Let $\mathcal{H}_B$ be a set of bound hypothesis as we have $\mathcal{H}_B = \brackc{h \in \mathcal{H} : L_{\mathcal{D}, f^*}(h)>\varepsilon}$ and let $M$ be the set of misleading samples: $\brackc{S|_x : \exists h \in \mathcal{H}_B, L_S(h) = 0}$ Observe that:
    \begin{equation*}
        \brackc{S|_x : \exists h \in \mathcal{H}_B, L_S(h) = 0} \subseteq M = \bigcup_{h\in\mathcal{H}_B }\brackc{S|_x : L_S(h) = 0}
    \end{equation*}
    Applying the union bound as we have the following union bound:
    \begin{equation*}
    \begin{aligned}
        \mathcal{D}^m(&\brackc{S|_x : \exists h \in \mathcal{H}_B, L_S(h) = 0}) \\
        &\le\sum_{h\in\mathcal{H}_B} \mathcal{D}^m(\brackc{S|_x : L_S(h) = 0} ) \\
        &\le |\mathcal{H}_B|\max_{h\in\mathcal{H}_B}\mathcal{D}^m(\brackc{S|_x : L_S(h) = 0}) \\
        &<|\mathcal{H}_B|(1-\varepsilon)^m \le \abs{\mathcal{H}}\exp(-\varepsilon m)
    \end{aligned}
    \end{equation*}
    Observe that $\mathcal{D}^m(\brackc{S|_x : L_S(h) = 0}) = (1-L_{\mathcal{D}, f^*}(h))^m$ if $h \in \mathcal{H}_B$, then $L_{\mathcal{D}, f^*}(h)\ge\varepsilon$. This leads to the third inequality, while the last inequality, we have: $1-\varepsilon \le\exp(-\varepsilon)$ and $|\mathcal{H}_B|\le\abs{\mathcal{H}}$. Setting the rhs to $\le\delta$ and we get the required inequality.
\end{proof}

\begin{definition}{\textbf{(PAC-Lernability)}}
    A hypothesis class $\mathcal{H}$ is PAC-learnable if there exists a function $m_\mathcal{H}:(0, 1)^2 \rightarrow \mathbb{N}$ and has the property of that for every $\varepsilon$ and $\delta \in (0, 1)$ and every distribution $\mathcal{D}$ over $\mathcal{X}$ and for every labeling function $f^* : \mathcal{X}\rightarrow \brackc{0, 1}$.
    \begin{itemize}
        \item Using the training algorithm $m \ge m_\mathcal{H}(\varepsilon, \delta)$ iid examples generated by $\mathcal{D}$ and labeled by $f^*$
        \item The algorithm returns a hypothesis $h$ such that with probability of at least $1-\delta$, the loss is $L_{\mathcal{D}, f^*}(h)\le \varepsilon$. 
        \item We call $m_\mathcal{H}$ is the sample complexity of the training hypothesis $\mathcal{H}$
    \end{itemize}
\end{definition}

\begin{remark}
    We are now interested in the infinite hypothesis space. What is the sample complexity of a given class ? Is there a generic algorithm that achieves the optimal sample complexity ?
\end{remark}

\begin{remark}{\textbf{(VC-Dimension: Motivation)}}
    Suppose, we have the training set: $S = (x_1, y_1),\dots,(x_m,y_m)$. We try to explain the label using a hypothesis from $\mathcal{H}$. We may get difference labels:
    \begin{equation*}
        (x_1,y_1'),\cdots,(x_m, y_m')
    \end{equation*}
    We can try to explain the label using a hypothesis from $\mathcal{H}$. If this works for us, no matter the labels are then, no free-lunch thoerem apply, as now we can't learn from $m/2$ example. 
\end{remark}

\begin{definition}{\textbf{(VC-Dimension)}}
    Let $C = \{x_1,\dots,x_{|C|}\} \subset \mathcal{X}$ . Let $\mathcal{H}_C$ is the restriction of $\mathcal{H}$ to $C$, then we have:
    \begin{equation*}
        \mathcal{H}_C = \brackc{h_C : h\in\mathcal{H}} \quad \text{ where } \quad h_C : C\rightarrow\brackc{-1, 1}
    \end{equation*}
    is such that $h_C(x_i) = h(x_i)$. For every $x_i \in C$, we can represent each $h_C$ as the vector:
    \begin{equation*}
        \mathcal{H}_C = \brackc{(h(x_1),\dots,h(x_{|C|}))\in\brackc{-1, 1}^{|C|}}
    \end{equation*}
    and so we have $\abs{\mathcal{H}_C}\le 2^{|C|}$ . We say that $\mathcal{H}$ shatters $C$ if $\abs{\mathcal{H}_C} = 2^{|C|}$ where we have:
    \begin{equation*}
        \operatorname{VCDim}(\mathcal{H}) = \sup\brackc{|C| : \mathcal{H} \text{ shatters } C}
    \end{equation*}
    VC dimension is the maximum size of a set $C$ such that $\mathcal{H}$ gives no prior knowledge with respected to $C$. 
\end{definition}

\begin{remark}
    To show that the VC dimension $\operatorname{VCDim}(\mathcal{H}) = d$, we have to show that:
    \begin{itemize}
        \item There exists a set $C$ of size $d$ which is shattered by $H$
        \item Every set $C$ of size $d+1$ isn't shattered by $\mathcal{H}$
    \end{itemize}
\end{remark}

\begin{proposition}{\textbf{(VC-Dimension of Intervals)}}
    Interval where we have $\mathcal{H} = \mathbb{R}$ and 
    \begin{equation*}
        \mathcal{H} = \brackc{h_{a, b} : a < b \in \mathbb{R}}
    \end{equation*} 
    where $h_{a, b}(x) = 1$ iff $x \in [a, b]$. Its VC-Dimension is $2$. 
\end{proposition}

\begin{proposition}{\textbf{(Axis Aligned Rectangle)}}
    We have $\mathcal{X} = \mathbb{R}^2$ as we have the hypothesis set to be:
    \begin{equation*}
        \mathcal{H} = \brackc{h_{(a_1,a_2,b_1,b_2)} : a_1<a_2 \text{ and } b_1<b_2}
    \end{equation*}
    where we have $h_{(a_1,a_2,b_1,b_2)}(\boldsymbol x_1, \boldsymbol x_2) = 1$ iff $\boldsymbol x_1 \in [a_1, a_2]$ and $\boldsymbol x_2 \in [b_1, b_2]$. We can show that $\operatorname{VCDim}(\mathcal{H})= 4$
\end{proposition}
\begin{proof}
    We can find $4$ points that can be shattered by $H$, and so $\operatorname{VCDim}(\mathcal{H}) \ge 4$. For any poitn $C\subseteq \mathbb{R}^2$ with $5$ points with label $(1,1,1,1,0)$ where $0$ is the point in the middle, we can't obtain any axis aligned rectangle, thus it can't be shattered $C$. Therefore, $\operatorname{VCDim}(\mathcal{H}) = 5$
\end{proof}

\begin{proposition}{\textbf{(Finite Class)}}
    The VC-Dimension of the finite $\mathcal{H}$ is at most $\log_2(\abs{\mathcal{H}})$ as there can arbitrary gaps between $\operatorname{VCDim}(\mathcal{H})$ and $\log_2(\abs{\mathcal{H}})$
\end{proposition}
\begin{proof}
    Let $\mathcal{H}$ be a finite class, for any set $C$ that can be shattered, we have $2^{|C|} = |\mathcal{H}_C| \le \abs{\mathcal{H}}$, thus the upperbound of the VC dimension is $\log_2\abs{\mathcal{H}}$
\end{proof}

\begin{theorem}{\textbf{(Radon)}}
    Any set $\mathcal{X}$ of $d+2$ data point $\mathbb{R}^d$ can be partion into $2$ sets $\mathcal{X}_1$ and $\mathcal{X}_2$ such that the convex hull of $\mathcal{X}_1$ and $\mathcal{X}_2$ intersect.
\end{theorem}
\begin{proof}
    Let $\mathcal{X} = \brackc{\boldsymbol x_1, \boldsymbol x_2, \dots, \boldsymbol x_{d+2}} \subset \mathbb{R}^d$ with the following linear equation:
    \begin{equation*}
        \sum^{d+2}_{i=1}\alpha_i\boldsymbol x_i = 0 \qquad \sum^{d+2}_{i=1} \alpha_i = 0
    \end{equation*}
    The number of unknown $d+2$ is larger than the number of equations $d+1$. This implies that the system admits non-zero solution $\beta_1,\dots,\beta_{\alpha+2}$ since $\sum^{d+2}_{i=1} \beta_i = 0 $ both:
    \begin{equation*}
        \mathcal{J}_1 = \brackc{i \in [d+2] : \beta_j > 0} \qquad \mathcal{J}_2 = \brackc{i \in [d+2] : \beta_j \le 0} 
    \end{equation*} 
    This means that $\mathcal{X}_1 = \brackc{x_i : i \in \mathcal{J}_1}$ and $\mathcal{X}_2 = \brackc{x_i : i \in \mathcal{J}_2}$ form a partition. The last equation gives us:
    \begin{equation*}
        \sum_{i\in\mathcal{J}_1}\beta_i = -\sum_{i\in\mathcal{J}_2}\beta_j
    \end{equation*}
    Let $\beta=\sum_{i\in\mathcal{J}_1}\beta_i$, then the first equation implies that:
    \begin{equation*}
        \sum_{i\in\mathcal{J}_1} \frac{\beta_i}{\beta}\boldsymbol x_i = -\sum_{i\in\mathcal{J}_2}\frac{\beta_i}{\beta}\boldsymbol x_i
    \end{equation*}
    Please note that: $\sum_{i\in\mathcal{J}_1} \beta_i/\beta = -\sum_{i\in\mathcal{J}_2}\beta_i/\beta = 1$ and $\beta_i/\beta\ge0$ for $i \in\mathcal{J}_1$ and $-\beta_j/\beta \ge 0$ for $i \in \mathcal{J}_2$. By the definition of the convex hull, this implies that $\sum_{i\in\mathcal{J}_1}\beta_i/\beta\boldsymbol x_i$ being both to convex hull $\mathcal{X}_1$ and $\mathcal{X}_2$
\end{proof}

\begin{proposition}{\textbf{(Hyperplane)}}
    We have $\mathcal{X} = \mathbb{R}^n$  and the hypothesis class to be:
    \begin{equation*}
        \mathcal{H} = \brackc{y \mapsto \operatorname{sgn}(\brackd{\boldsymbol w, \boldsymbol x}) : \boldsymbol w \in \mathbb{R}^n}
    \end{equation*}
    Then, we have $\operatorname{VCDim}(\mathcal{H}) = n+1$
\end{proposition}
\begin{proof}
    Starting with the lower bound, setting $\boldsymbol x_0$ to be the origin and setting $\boldsymbol x_i$ for $i\in[d]$ as the whose $i$ coordinate to be $1$ and all the others are $0$.
    \begin{itemize}
        \item Let $y_0, y_1,\dots,y_d \in \brackc{-1, 1}$ be an arbitrary set of label. 
        \item Let $\boldsymbol w$ be the vector whose $i$-th coordinate is $y_i$. 
    \end{itemize}
    The classifier defined by the hyperplane of equation $\boldsymbol w^T\boldsymbol x + y_0/2 = 0$, shatters $\boldsymbol x_0,\boldsymbol x_1, \dots,\boldsymbol x_d$ we can see that for any $i\in\brackc{0, \dots,d}$ as we have:
    \begin{equation*}
        \operatorname{sgn}\bracka{\boldsymbol w^T\boldsymbol x_i + \frac{y_0}{2}} = \operatorname{sgn}\bracka{y_i + \frac{y_0}{2}} = y_i
    \end{equation*}
    For the upperbound, let $\mathcal{X}$ be set of $d+2$ points. By Radon's theorem, it can be partition into $2$ sets $\mathcal{X}_1$ and $\mathcal{X}_2$ such that the convex hull intersects. When the set of points $\mathcal{X}_1$ and $\mathcal{X}_2$ are separated by hyperplane, the convex hull also separated. However, it is a contradiction and so the VC dimension is proven.
\end{proof}

\begin{definition}{\textbf{(Inner Production Space)}}
    The space is the bounded sequence summable square:
    \begin{equation*}
        l_2 = \brackc{\boldsymbol x \in \mathbb{R}^\infty : \sum^\infty_{i=1}x_i^2 < \infty}
    \end{equation*} 
    with the inner product to be $\brackc{\boldsymbol x, \boldsymbol x'} = \sum^\infty_{i=1}x_ix_i'$
\end{definition}

\begin{definition}{\textbf{(Large Margin Halfspaces)}}
    Given $\mathcal{X}\subset l_2$ and $\Lambda\in(0, \infty)$, which we define:
    \begin{equation*}
        \mathcal{H}_{\mathcal{X},\Lambda} = \brackc{\boldsymbol x\mapsto \operatorname{sgn}(\brackd{\boldsymbol w, \boldsymbol x}) : \boldsymbol x\in\mathcal{X}, \boldsymbol w \in l_2, \norm{\boldsymbol w} \le \Lambda, \brackd{\boldsymbol w, \boldsymbol x} \ge 1} 
    \end{equation*} 
    Observe that $1/\norm{\boldsymbol w}$ is the margin.
\end{definition}

\begin{theorem}
    We can show that for large margin halfspace:
    \begin{equation*}
        \operatorname{VCDim}(\mathcal{H}_{\mathcal{X},\Lambda}) \le \Lambda^2\max\norm{\boldsymbol x}^2_{\boldsymbol x\in \mathcal{X}}
    \end{equation*}
\end{theorem}

\begin{theorem}{\textbf{(Fundamental Theorem of Statistical Learning)}}
    Let $\mathcal{H}$ be a hypothesis class of binary classifier. Then there are absolute constant $C_1$ and $C_2$ such that the sample complexity is given by:
    \begin{equation*}
        C_1\frac{\operatorname{VCDim}(\mathcal{H}) + \log(1/\delta)}{\varepsilon} \le m_\mathcal{H}(\varepsilon, \delta) \le C_2\frac{\operatorname{VCDim}(\mathcal{H})\log(1/\varepsilon)+ \log(1/\delta)}{\varepsilon}
    \end{equation*}
    This sample complexity is achieved by ERM learning rule.
\end{theorem}

\subsection{Agnostic PAC-Learning}

\begin{remark}{\textbf{(Motivation for Agnostic PAC)}}
    Assuming that there exists $f^*$ may be too strong, so we relaxed the notation, so we use the assumption that the joint distribution $\mathcal{D}$ be a distribution over $\mathcal{X}\times\mathcal{Y}$ as now we are going to use:
    \begin{equation*}
        L_\mathcal{D}(h) = \mathbb{P}_{(x, y)\sim\mathcal{D}}[h(x) \ne y] := \mathcal{D}(\brackc{(x, y) : h(x) \ne y})
    \end{equation*}
    We will redefine the approximately correct notion.
\end{remark}

\begin{definition}{\textbf{(General Agnostic PAC)}}
    A hypothesis class $\mathcal{H}$ is agnostic PAC learnable if there exists a function $m_\mathcal{H}:(0, 1)^2\rightarrow \mathbb{N}$ and a learning algorithm $A$ with the following properties: for every $\delta, \varepsilon \in (0, 1)$ and $m>m_\mathcal{H}(\varepsilon,\delta)$:
    \begin{equation*}
        \mathcal{D}^m\bracka{\brackc{S \in (\mathcal{X}\times\mathcal{Y})^m : L_\mathcal{D}(A(S)) \le \min_{h\in\mathcal{H}}L_D(h) + \varepsilon }}\ge1-\delta
    \end{equation*}
\end{definition}

\begin{definition}{\textbf{($\boldsymbol \varepsilon$-Representation Sample)}}
    A training set $S$ is called $\varepsilon$-representative if for all $h \in \mathcal{H}$  as we have:
    \begin{equation*}
        \abs{L_S(h) - L_\mathcal{D}(S)} \le \varepsilon
    \end{equation*}
\end{definition}

\begin{lemma}
    Assume that a training set $S$ is $\varepsilon/2$-representative, then the average output of $\operatorname{ERM}_\mathcal{H}(S)$  namely $h_S \in \arg\min_{h\in\mathcal{H}}L_S(h)$ satsifies:
    \begin{equation*}
        L_\mathcal{D}(h_S) \le \min_{h\in\mathcal{H}}L_\mathcal{D}(h) + \varepsilon
    \end{equation*}
\end{lemma}
\begin{proof}
    For every $h\in\mathcal{H}$ as we have:
    \begin{equation*}
        L_\mathcal{D}(h_S) \le L_S(h_S) + \frac{\varepsilon}{2} \le L_S(h) + \frac{\varepsilon}{2} \le L_\mathcal{D}(h) + \frac{\varepsilon}{2} + \frac{\varepsilon}{2} =  L_\mathcal{D}(h)+\varepsilon
    \end{equation*}
\end{proof}

\begin{definition}{\textbf{(Uniform Convergence)}}
    Let $\mathcal{H}$ has the uniform convergence if there exists a function $m^\text{UC}_\mathcal{H} : (0, 1)^2 \rightarrow \mathbb{N}$ such that for every $\varepsilon, \delta\in(0, 1)$ and every distribution $\mathcal{D}$, and we have:
    \begin{equation*}
        \mathcal{D}^m\brackc{\Big( S \in Z^m : S \text{ is } \varepsilon-\text{representable}  \Big)} \ge1-\delta
    \end{equation*}
    where $Z$ is the domain and $m \ge m^\text{UC}_\mathcal{H} : (0, 1)^2$
\end{definition}

\begin{corollary}
    From the definition of uniform convergence, we can show that:
    \begin{itemize}
        \item If $\mathcal{H}$ has uniform convergence property with a function $m^\text{UC}_\mathcal{H}$ then $\mathcal{H}$ is agnostic PAC learnable with sample complexity of
        \begin{equation*}
            m_\mathcal{H}(\varepsilon,\delta) \le m^\text{UC}_\mathcal{H}(\varepsilon/2,\delta)
        \end{equation*}
        This follows from the lemma above.
        \item We can show that $\operatorname{ERM}_\mathcal{H}$ is successful against PAC learner for $\mathcal{H}$.
    \end{itemize}
\end{corollary}

\begin{theorem}
    Assume $\mathcal{H}$ is finite, then $\mathcal{H}$ is agnostic PAC learnable using $\text{ERM}_\mathcal{H}$ algorithm with:
    \begin{equation*}
        m_\mathcal{H}(\varepsilon,\delta) = \left\lceil \frac{2\log(2\abs{\mathcal{H}}/\delta)}{\varepsilon^2} \right\rceil
    \end{equation*}
    Comparing the reliable case generalization, the error will decrease in $\sqrt{m}$ values as oppose to linear.
\end{theorem}

\begin{proof}
    It suffices to show that $\mathcal{H}$ has the uniform convergence property with:
    \begin{equation*}
        m^\text{UC}_\mathcal{H}(\varepsilon, \delta) \le \left\lceil \frac{2\log(2\abs{\mathcal{H}}/\delta)}{\varepsilon^2} \right\rceil
    \end{equation*}
    To show that the uniform convergence, we need to show that:
    \begin{equation*}
        \mathcal{D}^m(\brackc{S : \exists h \in \mathcal{H}, \abs{L_S(h) - L_\mathcal{D}(h)} > \varepsilon})\le\delta
    \end{equation*}
    Using the union bound, we can see that:
    \begin{equation*}
    \begin{aligned}
        \mathcal{D}^m(\brackc{S : \exists h \in \mathcal{H}, \abs{L_S(h) - L_\mathcal{D}(h)} > \varepsilon})\le\delta &= \mathcal{D}^m\bracka{\bigcup_{h\in\mathcal{H}} \brackc{S : |L_S(h) - L_\mathcal{D}(h)|}\ge\varepsilon }\le\delta \\
        &\le \sum_{h\in\mathcal{H}}\mathcal{D}^m(\brackc{S : |L_S(h) - L_\mathcal{D}(h)|\ge\varepsilon })\le\delta \\
        &\le 2\abs{\mathcal{H}}\exp(-2m\varepsilon^2)
    \end{aligned}
    \end{equation*}
    The last inequality is shown by Hoeffding inequality, setting the correc $m$, to finish the proof.
\end{proof}

\begin{remark}{\textbf{(Error Decomposition)}}
    Let $h_S = \operatorname{ERM}_\mathcal{H}(S)$, we can decompose the risk as:
    \begin{equation*}
        L_\mathcal{D}(h_S) = \mathcal{E}_\text{app} + \mathcal{E}_\text{est}
    \end{equation*}
    We have the following error:
    \begin{itemize}
        \item \emph{Approximation Error}: $\mathcal{E}_\text{app} = \min_{h\in\mathcal{H}}L_\mathcal{D}(h)$. How much risk do we need to restrict $\mathcal{H}$ ? This doesn't depend on $S$, while it decreases with the complexity of $\mathcal{H}$ increases. 
        \item \emph{Estimation Error}: $\mathcal{E}_\text{est} = L_\mathcal{D}(h_S) - \min_{h\in\mathcal{H}}L_\mathcal{D}(h)$. It is the result of $L_S$ being estimator of $L_\mathcal{D}$, while it decreases with size $S$ but increase with complexity of $\mathcal{H}$. 
    \end{itemize}
    This is bias and complexity: choosing $\mathcal{H}'\supset\mathcal{H}$ leads to decreases in $\mathcal{E}_\text{app}$ while $\mathcal{E}_\text{est}$ increases. 
\end{remark}

