\section{Kernel and Regression}

\subsection{Introduction}

\begin{definition}{\textbf{(Convex Set)}}
    A set $\mathcal{X}$ is convex if $\boldsymbol p, \boldsymbol q \in \mathcal{X}$ is convex if $\alpha \boldsymbol p + (1-\alpha)\boldsymbol q \in \mathcal{X}$
\end{definition}

\begin{definition}{\textbf{(Convex Function)}}
    A function $f : \mathcal{X}\rightarrow \mathbb{R}$ is convex iff for all $\boldsymbol p, \boldsymbol q \in \mathcal{X}$ in convex set and $\alpha\in(0, 1)$ as we have:
    \begin{equation*}
        f(\alpha \boldsymbol p + (1-\alpha)\boldsymbol q) \le \alpha f(\boldsymbol p) + (1-\alpha)f(\boldsymbol q)
    \end{equation*}
    A function $f$ is concave if $-f$ is convex. A function is \emph{strictly convex} if we replace $\le$ with $<$. 
\end{definition}

\begin{remark}{\textbf{(Various Comments on Convex Function)}}
    We have the following results on the convex function, as we have:
    \begin{itemize}
        \item If $f$ and $g$ are convex, then $f + g$ is convex. 
        \item If $f$ is convex and $g$ is affine (linear + constant) then $f(g(\cdot))$ is convex. 
        \item Suppose $\boldsymbol M$ is symmetric matrix, then $\boldsymbol M$ is positive semi-definite matrix iff $f(\boldsymbol x) = \boldsymbol x^T\boldsymbol M\boldsymbol x$ is convex. 
        \item Level set $\brackc{\boldsymbol x : f(\boldsymbol x) = c}$ where $c \in \mathbb{R}$ of convex function $f$ is convex.
        \item For $f: (a, b) \rightarrow \mathbb{R}$ if $f''\ge0$ then $f$ is convex.
        \item For $f: \mathcal{X} \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ if $\nabla^2 f(\boldsymbol x)\succeq \boldsymbol 0$ for all $\boldsymbol x \in \mathcal{X}$, then $f$ is convex.
    \end{itemize}
\end{remark}

\subsection{Ridge Regression}

\begin{definition}{\textbf{(Ridge Regression Problem)}}
    Given a function $f(\boldsymbol x) = \boldsymbol w^T\boldsymbol x$ with a dataset:
    \begin{equation*}
        \mathcal{S} = \brackc{(\boldsymbol x_1, y_1),\dots,(\boldsymbol x_m, y_m)} \subset \mathbb{R}^n \times \mathbb{R}
    \end{equation*}
    Assuming the dataset is generated by the unknown function $g$ i.e $(\boldsymbol x, g(\boldsymbol x))$. Then suppose that the vector $\boldsymbol x_i$ are linearly independent with $m=n$, then there is a unique solution, whose parameter $\boldsymbol w$ solves:
    \begin{equation*}
        \boldsymbol X \boldsymbol w = \boldsymbol y
    \end{equation*}
    where $\boldsymbol y = (y_1,\dots, y_m)^T$ and $\boldsymbol X = [\boldsymbol x_1,\dots, \boldsymbol x_m]^T \in \mathbb{R}^{m\times n}$. 
\end{definition}

\begin{definition}{\textbf{(Well-Posed)}}
    The solution/problem is called well-posed if: the solution exists, uniquem and depends continuously on the data. The regularized theory allows general framework to solve ill-posted problem (we can choose the term to penalize complex function).
\end{definition}

\begin{definition}{\textbf{(Regularized Empirical Error)}}
    We minimize the following regularized empirical error, which is given by:
    \begin{equation*}
    \begin{aligned}
        \mathcal{E}_{\text{emph},\lambda}(\boldsymbol w) &= \sum^m_{j=1}(y_i - \boldsymbol w^T\boldsymbol x_i)^2 + \lambda \sum^n_{i=1} w_i^2 \\
        &= (\boldsymbol y - \boldsymbol X\boldsymbol w)^T(\boldsymbol y-\boldsymbol X\boldsymbol w) + \lambda \norm{\boldsymbol w}^2_2
    \end{aligned}
    \end{equation*}
    We can see that the parameter $\lambda > 0$ defines the trade-off between error and the norm of vector $\boldsymbol w$ (which restricts the complexity of the model).
\end{definition}

\begin{proposition}
    Solving the regularized empirical error by setting its gradient to $\boldsymbol 0$, gives us:
    \begin{equation*}
        \boldsymbol w = (\boldsymbol X^T\boldsymbol X + \lambda \boldsymbol I_n)^{-1}\boldsymbol X^T\boldsymbol y
    \end{equation*}
    Furthermore, we can show that the weight $\boldsymbol w = \sum^m_{i=1}\alpha_i\boldsymbol x_i$ and the solution can be written as:
    \begin{equation*}
        f(\boldsymbol x) = \sum^m_{i=1} \alpha_i \boldsymbol x_i^T\boldsymbol x_i
    \end{equation*}
    where $\boldsymbol \alpha = (\boldsymbol X\boldsymbol X^T + \lambda \boldsymbol I_m)^{-1}\boldsymbol y$. This is called dual form, while $f(\boldsymbol x) = \boldsymbol w^T\boldsymbol x$ is called primal form.
\end{proposition}
\begin{proof}
    Starting with the derivative, we have:
    \begin{equation*}
        \nabla \mathcal{E}_{\text{emp}, \lambda} (\boldsymbol w) = -2\boldsymbol X^T(\boldsymbol y - \boldsymbol X\boldsymbol w) + 2\lambda\boldsymbol w = \boldsymbol 0
    \end{equation*}
    which implies the weight of the first form i.e $\boldsymbol w = (\boldsymbol X^T\boldsymbol X + \lambda \boldsymbol I_n)^{-1}\boldsymbol X^T\boldsymbol y$. Now, we can also see that:
    \begin{equation*}
        \boldsymbol w = \frac{\boldsymbol X^T(\boldsymbol y - \boldsymbol X\boldsymbol w)}{\lambda}
    \end{equation*}
    Assume the the dual form of the weight $\boldsymbol w = \sum^m_{i=1}\alpha_i\boldsymbol x_i$ as we have:
    \begin{equation*}
        \alpha_i = \frac{y_i - \boldsymbol w^T\boldsymbol x_i}{\lambda} = \frac{y_i - (\sum^m_{i=1}\alpha_i\boldsymbol x_i)^T\boldsymbol x_i}{\lambda}  
    \end{equation*}
    Now solving for the value of $y_i$, which we have:
    \begin{equation*}
    \begin{aligned}
        y &= \bracka{\sum^m_{j=1}\alpha_j\boldsymbol x_j}^T \boldsymbol x_i + \lambda\alpha_i \\
        &= \sum^m_{j=1}(\alpha \boldsymbol x_j^T\boldsymbol x_j + \lambda \alpha_j\delta_{ij}) =  \sum^m_{j=1}(\boldsymbol x_j^T\boldsymbol x_j + \lambda\delta_{ij}) \boldsymbol \alpha
    \end{aligned}
    \end{equation*}
    and so we have $(\boldsymbol X\boldsymbol X^T + \lambda \boldsymbol I_m)\boldsymbol \alpha = \boldsymbol y$
\end{proof}

\begin{remark}{\textbf{(Advantage of Dual Form)}}
    The dual form allow us to gain a computational advantage for both training and testing time:
    \begin{itemize}
        \item \emph{Training Time}: Solving $\boldsymbol w$ in the primal function requires $\mathcal{O}(mn^2 + n^3)$ operations while solving for dual form $\mathcal{O}(nm^2 + m^3)$ if $m\ll n$ then it is more efficient that primal. 
        \item \emph{Testing Time}: Computing $f(\boldsymbol x)$ in test vector $\boldsymbol x$ in the primal form requires $\mathcal{O}(n)$ operations but the dual form requires $\mathcal{O}(nm)$ operations.
    \end{itemize} 
\end{remark}

\subsection{Basis/Kernel Functions}

\begin{definition}{\textbf{(Basis/Feature Function)}}
    We have the function $\boldsymbol \phi : \mathbb{R}^n\rightarrow \mathbb{R}^N$ as we have:
    \begin{equation*}
        \boldsymbol \phi(\boldsymbol x) = \Big( \boldsymbol \phi_1(\boldsymbol x),\dots, \boldsymbol \phi_N(\boldsymbol x) \Big)^T
    \end{equation*} 
    for $\boldsymbol x\in \mathbb{R}^n$, where we call $\boldsymbol \phi_1,\dots,\boldsymbol \phi_N$ are called basis function and $\boldsymbol \phi(\boldsymbol x)$ is called feature vector, and feature space is defined by: $\brackc{\boldsymbol \phi(\boldsymbol x) : \boldsymbol x \in \mathbb{R}^n}$
\end{definition}

\begin{remark}
    We can use the feature map of the data instead of real data $\boldsymbol \phi(\boldsymbol x)$. This gives us the many advantages, for example:
    \begin{itemize}
        \item The map: $\boldsymbol \phi(\boldsymbol x) = (\boldsymbol x, 1)^T$ allow us to have the bias terms. 
        \item The map: $\boldsymbol \phi(\boldsymbol x) = (\boldsymbol x_1x_2)^T$ allow us to consider the interaction between inputs (individual elements). 
    \end{itemize}
    We can also consider the second order correlation if $\boldsymbol x\in \mathbb{R}^n$ as:
    \begin{equation*}
        \boldsymbol \phi(\boldsymbol x) = (x_1x_1,x_1x_2,\dots,x_1x_n,x_2x_2,x_2x_3,\dots,x_2x_n,\dots,x_nx_n)^T
    \end{equation*}
    now the feature vector has the size of $(n^2 + n)/2$. However, if we consider the inner product, we will have:
    \begin{equation*}
    \begin{aligned}
        \brackd{\boldsymbol \phi(\boldsymbol x), \boldsymbol \phi(\boldsymbol t)} &= (x_1x_1,x_1x_2,\dots,x_nx_n)^T(t_1t_1,t_1t_2,\dots,t_nt_n) \\
        &= (x_1t_1+\cdots + x_nt_n) (x_1t_1+\cdots + x_nt_n) \\
        &= (\boldsymbol x^T\boldsymbol t)^T
    \end{aligned}
    \end{equation*}
    Note that $\mathcal{O}(n)$ but the native computation will take $\mathcal{O}(n^2)$.  This leads to decrease the computation complexity (please see the dual form too). 
\end{remark}

\begin{definition}{\textbf{(Kernel Function)}}
    Given a feature map $\boldsymbol \phi$, we define the asssociated kernel function $k : \mathbb{R}^n\times \mathbb{R}^n \rightarrow \mathbb{R}$ as we have:
    \begin{equation*}
        k(\boldsymbol x, \boldsymbol t) = \brackd{\boldsymbol \phi(\boldsymbol x), \boldsymbol \phi(\boldsymbol t)}
    \end{equation*}
    Please note that the computing $k(\boldsymbol x, \boldsymbol t)$, which it doesn't depends on computing $\boldsymbol \phi(\boldsymbol x)$. 
\end{definition}

\begin{remark}{\textbf{(Feature Map not Unique)}}
    The feature map isn't unique. Consider the $\boldsymbol \phi$ that is associated with kernel $k$, and so $\hat{\boldsymbol \phi} = \boldsymbol U\boldsymbol \phi$ where $U \in \mathbb{R}^{N\times N}$. The feature can be difference in values and dimension but gives rise to the same kernel:
    \begin{equation*}
        (\boldsymbol U\boldsymbol \phi)^T(\boldsymbol U\boldsymbol \phi) = \boldsymbol \phi^T\boldsymbol \phi
    \end{equation*}
\end{remark}

\begin{theorem}{\textbf{(Representor)}}
    Consider the loss to be:
    \begin{equation*}
        \mathcal{E}_{\text{emp}, \lambda}(\boldsymbol w) = \sum^m_{i=1} V(y_i, \brackd{\boldsymbol w, \boldsymbol \phi(\boldsymbol x_i)}) + \lambda\brackd{\boldsymbol w, \boldsymbol w}
    \end{equation*}
    where $V: \mathbb{R}\times \mathbb{R}\rightarrow \mathbb{R}$ is a loss function. If $V$ is differentiable with respected to its second argument and $\boldsymbol w$ is a minimizer of $\mathcal{E}_\lambda$, then $\boldsymbol w$ has the form of:
    \begin{equation*}
        \boldsymbol w = \sum^m_{i=1}\alpha_i\boldsymbol \phi(\boldsymbol x_i) \implies f(\boldsymbol x) = \brackd{\boldsymbol w, \boldsymbol \phi(\boldsymbol x)} = \sum^m_{i=1}\alpha_ik(\boldsymbol x_i, \boldsymbol x)
    \end{equation*}
\end{theorem}
\begin{proof}
    The proof is similar to the dual form. Setting the derivative of $\mathcal{E}_\lambda$ with respected to zero and we have:
    \begin{equation*}
        \sum^m_{i=1}V'(y_i, \brackd{\boldsymbol w, \boldsymbol \phi(\boldsymbol x_i)})\boldsymbol \phi(\boldsymbol x_i) + 2\lambda\boldsymbol w = 0
    \end{equation*}
    Compared to $\boldsymbol w = \sum^m_{i=1}\alpha_i\boldsymbol \phi(\boldsymbol x_i)$, we can see that:
    \begin{equation*}
        \alpha_i = \frac{1}{2\lambda}V'(y_i, \brackd{\boldsymbol w, \boldsymbol \phi(\boldsymbol x_i)})
    \end{equation*}
    From the definition of $\boldsymbol w$, we can see that:
    \begin{equation*}
        \alpha_i = \frac{1}{2\lambda}V'\bracka{y_i, \sum^m_{j=1}k(\boldsymbol x_i, \boldsymbol x_j)\alpha_j}
    \end{equation*}
    for $i=1,\dots,m$. Finding $\boldsymbol \alpha$ is done by solving the following optimization problem
    \begin{equation*}
        \argmax{\boldsymbol \alpha} \sum^m_{i=1} V(y_i, (\boldsymbol K\boldsymbol \alpha)_i) + \boldsymbol \alpha^T\boldsymbol K\boldsymbol \alpha
    \end{equation*} 
\end{proof}

\begin{definition}{\textbf{(Positive Semi-Definite Kernel)}}
    The kernel $k : \mathbb{R}^n\times \mathbb{R}^n \rightarrow \mathbb{R}$ is positive semi-definite if it is symmetrix and given the set of points $\brackc{\boldsymbol x_1,\dots,\boldsymbol x_n}$, the matrix:
    \begin{equation*}
        \begin{bmatrix}
            k(\boldsymbol x_1,\boldsymbol x_1) & \cdots & k(\boldsymbol x_1, \boldsymbol x_n) \\
            \vdots & \ddots & \vdots \\
            k(\boldsymbol x_n,\boldsymbol x_1) & \cdots & k(\boldsymbol x_n, \boldsymbol x_n) \\
        \end{bmatrix}
    \end{equation*}
    is positive semi-definite. 
\end{definition}

\begin{theorem}
    Kernel $k$ is positive definite iff:
    \begin{equation*}
        k(\boldsymbol x, \boldsymbol y) = \brackd{\boldsymbol \phi(\boldsymbol x), \boldsymbol \phi(\boldsymbol t)}
    \end{equation*}
    for $\boldsymbol x, \boldsymbol t \in \mathbb{R}^n$ for some feature map $\boldsymbol \phi : \mathbb{R}^n \rightarrow \mathcal{W}$ for Hilber space $\mathcal{W}$
\end{theorem}
\begin{proof}
    We will consider only one direction. If $k(\boldsymbol x, \boldsymbol t) = \brackd{\boldsymbol \phi(\boldsymbol x), \boldsymbol \phi(\boldsymbol t)}$, then we have:
    \begin{equation*}
        \sum^n_{i=1}\sum^m_{j=1}c_ic_jk(\boldsymbol x_i, \boldsymbol x_j) = \brackd{\sum^m_{i=1}c_i\boldsymbol \phi(\boldsymbol x_i), \sum^m_{j=1}c_j\boldsymbol \phi(\boldsymbol x_j)} =\norm{\sum^m_{i=1}c_i\boldsymbol \phi(\boldsymbol x_i)}^2 \ge 0
    \end{equation*}
\end{proof}

\begin{definition}{\textbf{(Polynomial Kernel)}}
    If $p:\mathbb{R}\rightarrow \mathbb{R}$ is a polynomial with non-negative coefficient then $k(\boldsymbol x, \boldsymbol z) = p (\boldsymbol x^T\boldsymbol t)$ where $\boldsymbol x, \boldsymbol t \in \mathbb{R}^n$ and $k$ positive semi-definite kernel. 
\end{definition}

\begin{proposition}
    If $\boldsymbol A$ is an $n\times n$ positive semi-definite matrix, the function $k : \mathbb{R}^n\times \mathbb{R}^n \rightarrow \mathbb{R}$ defined by:
    \begin{equation*}
        k(\boldsymbol x, \boldsymbol t) = \boldsymbol x^T\boldsymbol A\boldsymbol t
    \end{equation*}
    is a generalized linear kernel and it is a positive semi-definite kernel.
\end{proposition}
\begin{proof}
    Since $\boldsymbol A$ is positive semi-definite, we can write $\boldsymbol A$ in the form of $\boldsymbol A=\boldsymbol R\boldsymbol R^T$ for some $\boldsymbol R \in \mathbb{R}^{n\times n}$. Thus, $k$ is represented by a feature map $\boldsymbol \phi(\boldsymbol x) = \boldsymbol R^T\boldsymbol x$. As we can see that:
    \begin{equation*}
    \begin{aligned}
        \sum_{ij} c_ic_j\boldsymbol x_i^T\boldsymbol A\boldsymbol x_j &= \sum_{ij}c_ic_j(\boldsymbol R^T\boldsymbol x_i)^T(\boldsymbol R^T\boldsymbol x_j) \\
        &= \sum_i c_i[\boldsymbol R^T\boldsymbol x_i]^T\brackb{\sum_j c_j(\boldsymbol R^T\boldsymbol x_j)} = \norm{\sum_i c_i\boldsymbol R^T\boldsymbol x_i}^2 \ge 0
    \end{aligned}
    \end{equation*}
\end{proof}

\begin{proposition}
    If $k:\mathbb{R}^N\times \mathbb{R}^N\rightarrow \mathbb{R}$ is a positve semi-definite kernel and $\boldsymbol \phi: \mathbb{R}^n \rightarrow \mathbb{R}^N$. 
    \begin{equation*}
        \tilde{k}(\boldsymbol x, \boldsymbol t) = k(\boldsymbol \phi(\boldsymbol x), \boldsymbol \phi(\boldsymbol t))
    \end{equation*}
    The kernel $\tilde{k}$ defined to be $\tilde{k}:\mathbb{R}^n\times \mathbb{R}^n \rightarrow \mathbb{R}$ is a positive definite kernel.
\end{proposition}

\begin{proposition}
    Given a positive semi-definite kernels $k_1$ and $k_2$, $ak_1$ is a positive semi-definite kernel if $a>0$ and $k_1 + k_2$ is also a positive definite kernel.
\end{proposition}

\begin{proposition}
    We consider the following combination of kernel $k_1$ and $k_2$ are given as:
    \begin{equation*}
        k(\boldsymbol x, \boldsymbol t) = k_1(\boldsymbol x, \boldsymbol t)k_2(\boldsymbol x, \boldsymbol t)
    \end{equation*}
    where $\boldsymbol x, \boldsymbol t \in \mathbb{R}^d$ is a kernel. 
\end{proposition}
\begin{proof}
    For the product of kernel, we have:
    \begin{itemize}
        \item We want to show that for positive semi-definite $\boldsymbol A$ and $\boldsymbol B$ where $\boldsymbol C = A\odot \boldsymbol B$ is a positive semi-definite. 
        \item Since $\boldsymbol A$ and $\boldsymbol B$ are positive semi-definite, where it can be factorized as $\boldsymbol A = \boldsymbol U\boldsymbol U^T$ and $\boldsymbol B = \boldsymbol V\boldsymbol V^T$ for $\boldsymbol U, \boldsymbol V \in \mathbb{R}^{n\times n}$ as we have:
        \begin{equation*}
        \begin{aligned}
            \sum^n_{i=1}\sum^n_{j=1} z_iz_jC_{ij} &= \sum^n_{i=1}\sum_{j=1}^n z_iz_j\bracka{\sum^n_{r=1} U_{ir}U_{jr}}\bracka{\sum^n_{s=1} V_{is}V_{js}} \\
            &= \sum^n_{i=1}\sum_{j=1}^n\sum^n_{r=1}\sum^n_{s=1}z_iz_jU_{ir}U_{jr}V_{is}V_{js} \\
            &= \sum^n_{r=1}\sum^n_{s=1}\sum^n_{i=1}\sum_{j=1}^nz_iz_jU_{ir}U_{jr}V_{is}V_{js} \\
            &= \sum^n_{r=1}\sum^n_{s=1}\sum_{i=1}^nz_iU_{ir}V_{is}\sum^n_{j=1}z_jU_{ji}V_{js} = \sum^n_{r=1}\sum^n_{s=1}\bracka{\sum_{i=1}^n z_iU_{ir}V_{is}}^2 \ge 0
        \end{aligned}
        \end{equation*}
        Thus complete the proof. This proves the polynomial kernel is positive definite kernel. 
    \end{itemize}
\end{proof}

\begin{remark}{\textbf{(Several Kernels)}}
    We have the following positve definite kernel, where we have $a\ge 0$:
    \begin{itemize}
        \item $k(\boldsymbol x, \boldsymbol t) = (\boldsymbol x^T\boldsymbol t)^r$
        \item $k(\boldsymbol x, \boldsymbol t) = (a + \boldsymbol x^T\boldsymbol t)^r$
        \item $k(\boldsymbol x, \boldsymbol t) = \sum^d_{i=1}(a^i/i!)(\boldsymbol x^T\boldsymbol t)^r$
        \item Gaussian Kernel: $k(\boldsymbol x, \boldsymbol t) = \exp(-\beta\norm{\boldsymbol x - \boldsymbol t}^2)$ for $\beta>0$ the data $\boldsymbol x, \boldsymbol t \in \mathbb{R}^n$ (It has infinite dimensional feature map)
        \item ANOVA kernel: $k(\boldsymbol x, \boldsymbol t) = \prod^n_{i=1}(1 + x_it_i)$
    \end{itemize}
\end{remark}

\begin{remark}
    Consider the following polynomial kernel as we have:
    \begin{equation*}
        \sum^d_{i=1} \frac{a^i}{i!}(\boldsymbol x^T\boldsymbol t)^i
    \end{equation*}
    Suppose we have $r=\infty$, this can converge uniformly to $\exp(a\boldsymbol x^T\boldsymbol t)$ showing that it is a kernel, where if $n=1$, the feature map is:
    \begin{equation*}
        \phi = \bracka{1 , \sqrt{2} x, \sqrt{\frac{a}{2}}x^2, \sqrt{\frac{a^3}{6}}x^3, \cdots} = \bracka{\sqrt{\frac{a^i}{i!}} : i \in \mathbb{N}}
    \end{equation*}
\end{remark}

\begin{definition}{\textbf{(Transition Invariance/Radial Kernel)}}
    We say that a kernel $k : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ is:
    \begin{itemize}
        \item \emph{Transition Invariance}: If the kernel has the form:
        \begin{equation*}
            k(\boldsymbol x, \boldsymbol t) = H(\boldsymbol x - \boldsymbol t)
        \end{equation*}
        for all $\boldsymbol x, \boldsymbol t \in \mathbb{R}^d$ where $H : \mathbb{R}^d \rightarrow \mathbb{R}$ is a differentiable function. 
        \item \emph{Radial}, if kernel has the form:
        \begin{equation*}
            k(\boldsymbol x, \boldsymbol t) = h(\norm{\boldsymbol x- \boldsymbol t})
        \end{equation*}
        for all $\boldsymbol x, \boldsymbol t \in \mathbb{R}^d$ where $h : [0, \infty)\rightarrow [0, \theta)$ is the differentiable function. 
    \end{itemize}
\end{definition}

\begin{remark}
    The important example of a radial kernel in the Gaussian kernel as we have:
    \begin{equation*}
        k(\boldsymbol x, \boldsymbol t) = \exp(-\beta\norm{\boldsymbol x- \boldsymbol t}^2)
    \end{equation*}
    which is a product of $2$ kernel as $k(\boldsymbol x, \boldsymbol t) = \exp(-\beta(\boldsymbol x^T\boldsymbol x + \boldsymbol t^T\boldsymbol t))\exp(2\beta\boldsymbol x^T\boldsymbol t)$
\end{remark}

\begin{remark}{\textbf{(Ridge Regression with Feature Map)}}
    Given the dataset $\boldsymbol X \in \mathbb{R}^{m\times n}$  and $\boldsymbol y \in \mathbb{R}^{m\times 1}$. Starting with the basis function $\phi_1,\dots,\phi_N$ where $\phi_i : \mathbb{R}^n \rightarrow \mathbb{R}$ with the map:
    \begin{equation*}
        \boldsymbol \Phi = \begin{bmatrix}
            \phi_1(\boldsymbol x_1) & \cdots & \phi_N(\boldsymbol x_1)  \\
            \vdots & \ddots & \vdots \\
            \phi_1(\boldsymbol x_m) & \cdots & \phi_N(\boldsymbol x_m)  \\
        \end{bmatrix} \in \mathbb{R}^{m\times N}
    \end{equation*}
    We have the regression coefficient as we have $\boldsymbol w = (\boldsymbol \Phi^T\boldsymbol \Phi + \lambda \boldsymbol I_N)^{-1}\boldsymbol \Phi^T\boldsymbol y$
\end{remark}

\begin{remark}{\textbf{(Kernel Ridge Regression)}}
    Given the same setting, a kernel function $\mathbb{R}^n\times \mathbb{R}^n \rightarrow \mathbb{R}$, where the kernel matrix is given by:
    \begin{equation*}
        \boldsymbol K = \begin{bmatrix}
            k(\boldsymbol x_1,\boldsymbol x_1) & \cdots & k(\boldsymbol x_1, \boldsymbol x_n) \\
            \vdots & \ddots & \vdots \\
            k(\boldsymbol x_n,\boldsymbol x_1) & \cdots & k(\boldsymbol x_n, \boldsymbol x_n) \\
        \end{bmatrix} \in \mathbb{R}^{m\times m}
    \end{equation*}
    Regression coefficient is then given by $\boldsymbol \alpha = (\boldsymbol K + \lambda \boldsymbol I_m)^{-1}\boldsymbol y$ as the function is:
    \begin{equation*}
        \hat{y}(\boldsymbol x) = \sum^m_{i=1}\alpha_i k(\boldsymbol x_i, \boldsymbol x)
    \end{equation*}
\end{remark}
